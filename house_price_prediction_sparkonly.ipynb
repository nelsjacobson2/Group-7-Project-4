{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "600d2144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/07 11:05:47 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.69 instead (on interface en0)\n",
      "23/08/07 11:05:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/07 11:05:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import necessary libraries and create Spark session\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"HousePricePrediction\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286e64b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Read the data\n",
    "df_train = spark.read.csv(\"data/train.csv\", header=True, inferSchema=True)\n",
    "df_test = spark.read.csv(\"data/test.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bceab782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Cell 3: Drop unnecessary columns and handle missing values for \"MSZoning\" column\n",
    "cols_to_drop = ['FireplaceQu', 'Fence', 'Alley', 'MiscFeature', 'PoolQC']\n",
    "df_train_cleaned = df_train.drop(*cols_to_drop)\n",
    "df_test_cleaned = df_test.drop(*cols_to_drop)\n",
    "\n",
    "mszoning_mode = df_train_cleaned.select(\"MSZoning\").groupBy(\"MSZoning\").count().orderBy(col(\"count\").desc()).first()[\"MSZoning\"]\n",
    "df_train_cleaned = df_train_cleaned.na.fill({\"MSZoning\": mszoning_mode})\n",
    "df_test_cleaned = df_test_cleaned.na.fill({\"MSZoning\": mszoning_mode})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a60eaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/07 11:07:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Handle missing values for both categorical and numerical features\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "categorical_cols = [col_name for col_name, dtype in df_train_cleaned.dtypes if dtype == \"string\"]\n",
    "for col in categorical_cols:\n",
    "    mode_value = df_train_cleaned.select(col).groupBy(col).count().orderBy(F.col(\"count\").desc()).first()[col]\n",
    "    df_train_cleaned = df_train_cleaned.na.fill({col: mode_value})\n",
    "    df_test_cleaned = df_test_cleaned.na.fill({col: mode_value})\n",
    "\n",
    "numerical_cols = [col_name for col_name, dtype in df_train_cleaned.dtypes if dtype != \"string\" and col_name != \"Id\" and col_name != \"SalePrice\"]\n",
    "for col in numerical_cols:\n",
    "    df_train_cleaned = df_train_cleaned.withColumn(col, F.col(col).cast(\"double\"))\n",
    "    df_test_cleaned = df_test_cleaned.withColumn(col, F.col(col).cast(\"double\"))\n",
    "\n",
    "imputer = Imputer(inputCols=numerical_cols, outputCols=[f\"{col}_imputed\" for col in numerical_cols])\n",
    "imputer_model = imputer.fit(df_train_cleaned)\n",
    "df_train_cleaned = imputer_model.transform(df_train_cleaned)\n",
    "df_test_cleaned = imputer_model.transform(df_test_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034930d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Convert columns to the correct data types (after filling missing values for categorical columns)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "for col in df_train_cleaned.columns:\n",
    "    if col != \"Id\" and col != \"SalePrice\":\n",
    "        df_train_cleaned = df_train_cleaned.withColumn(col, F.col(col).cast(\"double\"))\n",
    "        df_test_cleaned = df_test_cleaned.withColumn(col, F.col(col).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3402f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Drop columns with a high percentage of missing values\n",
    "missing_threshold = 0.8\n",
    "cols_to_drop = [col for col in df_train_cleaned.columns if (df_train_cleaned.select(col).na.drop().count() / df_train_cleaned.count()) < missing_threshold]\n",
    "df_train_cleaned = df_train_cleaned.drop(*cols_to_drop)\n",
    "df_test_cleaned = df_test_cleaned.drop(*cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed4faba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/07 11:21:08 ERROR Executor: Exception in task 0.0 in stage 790.0 (TID 534)\n",
      "org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3943/0x0000000801b4dd48: (struct<MSSubClass:double,LotFrontage:double,LotArea:double,OverallQual:double,OverallCond:double,YearBuilt:double,YearRemodAdd:double,MasVnrArea:double,BsmtFinSF1:double,BsmtFinSF2:double,BsmtUnfSF:double,TotalBsmtSF:double,1stFlrSF:double,2ndFlrSF:double,LowQualFinSF:double,GrLivArea:double,BsmtFullBath:double,BsmtHalfBath:double,FullBath:double,HalfBath:double,BedroomAbvGr:double,KitchenAbvGr:double,TotRmsAbvGrd:double,Fireplaces:double,GarageYrBlt:double,GarageCars:double,GarageArea:double,WoodDeckSF:double,OpenPorchSF:double,EnclosedPorch:double,3SsnPorch:double,ScreenPorch:double,PoolArea:double,MiscVal:double,MoSold:double,YrSold:double,MSSubClass_imputed:double,LotArea_imputed:double,OverallQual_imputed:double,OverallCond_imputed:double,YearBuilt_imputed:double,YearRemodAdd_imputed:double,BsmtFinSF1_imputed:double,BsmtFinSF2_imputed:double,BsmtUnfSF_imputed:double,TotalBsmtSF_imputed:double,1stFlrSF_imputed:double,2ndFlrSF_imputed:double,LowQualFinSF_imputed:double,GrLivArea_imputed:double,BsmtFullBath_imputed:double,BsmtHalfBath_imputed:double,FullBath_imputed:double,HalfBath_imputed:double,BedroomAbvGr_imputed:double,KitchenAbvGr_imputed:double,TotRmsAbvGrd_imputed:double,Fireplaces_imputed:double,GarageCars_imputed:double,GarageArea_imputed:double,WoodDeckSF_imputed:double,OpenPorchSF_imputed:double,EnclosedPorch_imputed:double,3SsnPorch_imputed:double,ScreenPorch_imputed:double,PoolArea_imputed:double,MiscVal_imputed:double,MoSold_imputed:double,YrSold_imputed:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:161)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 26 more\n",
      "23/08/07 11:21:09 WARN TaskSetManager: Lost task 0.0 in stage 790.0 (TID 534) (192.168.1.69 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3943/0x0000000801b4dd48: (struct<MSSubClass:double,LotFrontage:double,LotArea:double,OverallQual:double,OverallCond:double,YearBuilt:double,YearRemodAdd:double,MasVnrArea:double,BsmtFinSF1:double,BsmtFinSF2:double,BsmtUnfSF:double,TotalBsmtSF:double,1stFlrSF:double,2ndFlrSF:double,LowQualFinSF:double,GrLivArea:double,BsmtFullBath:double,BsmtHalfBath:double,FullBath:double,HalfBath:double,BedroomAbvGr:double,KitchenAbvGr:double,TotRmsAbvGrd:double,Fireplaces:double,GarageYrBlt:double,GarageCars:double,GarageArea:double,WoodDeckSF:double,OpenPorchSF:double,EnclosedPorch:double,3SsnPorch:double,ScreenPorch:double,PoolArea:double,MiscVal:double,MoSold:double,YrSold:double,MSSubClass_imputed:double,LotArea_imputed:double,OverallQual_imputed:double,OverallCond_imputed:double,YearBuilt_imputed:double,YearRemodAdd_imputed:double,BsmtFinSF1_imputed:double,BsmtFinSF2_imputed:double,BsmtUnfSF_imputed:double,TotalBsmtSF_imputed:double,1stFlrSF_imputed:double,2ndFlrSF_imputed:double,LowQualFinSF_imputed:double,GrLivArea_imputed:double,BsmtFullBath_imputed:double,BsmtHalfBath_imputed:double,FullBath_imputed:double,HalfBath_imputed:double,BedroomAbvGr_imputed:double,KitchenAbvGr_imputed:double,TotRmsAbvGrd_imputed:double,Fireplaces_imputed:double,GarageCars_imputed:double,GarageArea_imputed:double,WoodDeckSF_imputed:double,OpenPorchSF_imputed:double,EnclosedPorch_imputed:double,3SsnPorch_imputed:double,ScreenPorch_imputed:double,PoolArea_imputed:double,MiscVal_imputed:double,MoSold_imputed:double,YrSold_imputed:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:161)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 26 more\n",
      "\n",
      "23/08/07 11:21:09 ERROR TaskSetManager: Task 0 in stage 790.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2956.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 790.0 failed 1 times, most recent failure: Lost task 0.0 in stage 790.0 (TID 534) (192.168.1.69 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3943/0x0000000801b4dd48: (struct<MSSubClass:double,LotFrontage:double,LotArea:double,OverallQual:double,OverallCond:double,YearBuilt:double,YearRemodAdd:double,MasVnrArea:double,BsmtFinSF1:double,BsmtFinSF2:double,BsmtUnfSF:double,TotalBsmtSF:double,1stFlrSF:double,2ndFlrSF:double,LowQualFinSF:double,GrLivArea:double,BsmtFullBath:double,BsmtHalfBath:double,FullBath:double,HalfBath:double,BedroomAbvGr:double,KitchenAbvGr:double,TotRmsAbvGrd:double,Fireplaces:double,GarageYrBlt:double,GarageCars:double,GarageArea:double,WoodDeckSF:double,OpenPorchSF:double,EnclosedPorch:double,3SsnPorch:double,ScreenPorch:double,PoolArea:double,MiscVal:double,MoSold:double,YrSold:double,MSSubClass_imputed:double,LotArea_imputed:double,OverallQual_imputed:double,OverallCond_imputed:double,YearBuilt_imputed:double,YearRemodAdd_imputed:double,BsmtFinSF1_imputed:double,BsmtFinSF2_imputed:double,BsmtUnfSF_imputed:double,TotalBsmtSF_imputed:double,1stFlrSF_imputed:double,2ndFlrSF_imputed:double,LowQualFinSF_imputed:double,GrLivArea_imputed:double,BsmtFullBath_imputed:double,BsmtHalfBath_imputed:double,FullBath_imputed:double,HalfBath_imputed:double,BedroomAbvGr_imputed:double,KitchenAbvGr_imputed:double,TotRmsAbvGrd_imputed:double,Fireplaces_imputed:double,GarageCars_imputed:double,GarageArea_imputed:double,WoodDeckSF_imputed:double,OpenPorchSF_imputed:double,EnclosedPorch_imputed:double,3SsnPorch_imputed:double,ScreenPorch_imputed:double,PoolArea_imputed:double,MiscVal_imputed:double,MoSold_imputed:double,YrSold_imputed:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:161)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3943/0x0000000801b4dd48: (struct<MSSubClass:double,LotFrontage:double,LotArea:double,OverallQual:double,OverallCond:double,YearBuilt:double,YearRemodAdd:double,MasVnrArea:double,BsmtFinSF1:double,BsmtFinSF2:double,BsmtUnfSF:double,TotalBsmtSF:double,1stFlrSF:double,2ndFlrSF:double,LowQualFinSF:double,GrLivArea:double,BsmtFullBath:double,BsmtHalfBath:double,FullBath:double,HalfBath:double,BedroomAbvGr:double,KitchenAbvGr:double,TotRmsAbvGrd:double,Fireplaces:double,GarageYrBlt:double,GarageCars:double,GarageArea:double,WoodDeckSF:double,OpenPorchSF:double,EnclosedPorch:double,3SsnPorch:double,ScreenPorch:double,PoolArea:double,MiscVal:double,MoSold:double,YrSold:double,MSSubClass_imputed:double,LotArea_imputed:double,OverallQual_imputed:double,OverallCond_imputed:double,YearBuilt_imputed:double,YearRemodAdd_imputed:double,BsmtFinSF1_imputed:double,BsmtFinSF2_imputed:double,BsmtUnfSF_imputed:double,TotalBsmtSF_imputed:double,1stFlrSF_imputed:double,2ndFlrSF_imputed:double,LowQualFinSF_imputed:double,GrLivArea_imputed:double,BsmtFullBath_imputed:double,BsmtHalfBath_imputed:double,FullBath_imputed:double,HalfBath_imputed:double,BedroomAbvGr_imputed:double,KitchenAbvGr_imputed:double,TotRmsAbvGrd_imputed:double,Fireplaces_imputed:double,GarageCars_imputed:double,GarageArea_imputed:double,WoodDeckSF_imputed:double,OpenPorchSF_imputed:double,EnclosedPorch_imputed:double,3SsnPorch_imputed:double,ScreenPorch_imputed:double,PoolArea_imputed:double,MiscVal_imputed:double,MoSold_imputed:double,YrSold_imputed:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:161)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rl/kx3s6v3x2xd0bdr07yj5mmkm0000gn/T/ipykernel_88271/1434707155.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"numerical_features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"scaled_numerical_features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithMean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithStd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mscaler_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_assembled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdf_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_assembled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdf_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test_assembled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2956.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 790.0 failed 1 times, most recent failure: Lost task 0.0 in stage 790.0 (TID 534) (192.168.1.69 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3943/0x0000000801b4dd48: (struct<MSSubClass:double,LotFrontage:double,LotArea:double,OverallQual:double,OverallCond:double,YearBuilt:double,YearRemodAdd:double,MasVnrArea:double,BsmtFinSF1:double,BsmtFinSF2:double,BsmtUnfSF:double,TotalBsmtSF:double,1stFlrSF:double,2ndFlrSF:double,LowQualFinSF:double,GrLivArea:double,BsmtFullBath:double,BsmtHalfBath:double,FullBath:double,HalfBath:double,BedroomAbvGr:double,KitchenAbvGr:double,TotRmsAbvGrd:double,Fireplaces:double,GarageYrBlt:double,GarageCars:double,GarageArea:double,WoodDeckSF:double,OpenPorchSF:double,EnclosedPorch:double,3SsnPorch:double,ScreenPorch:double,PoolArea:double,MiscVal:double,MoSold:double,YrSold:double,MSSubClass_imputed:double,LotArea_imputed:double,OverallQual_imputed:double,OverallCond_imputed:double,YearBuilt_imputed:double,YearRemodAdd_imputed:double,BsmtFinSF1_imputed:double,BsmtFinSF2_imputed:double,BsmtUnfSF_imputed:double,TotalBsmtSF_imputed:double,1stFlrSF_imputed:double,2ndFlrSF_imputed:double,LowQualFinSF_imputed:double,GrLivArea_imputed:double,BsmtFullBath_imputed:double,BsmtHalfBath_imputed:double,FullBath_imputed:double,HalfBath_imputed:double,BedroomAbvGr_imputed:double,KitchenAbvGr_imputed:double,TotRmsAbvGrd_imputed:double,Fireplaces_imputed:double,GarageCars_imputed:double,GarageArea_imputed:double,WoodDeckSF_imputed:double,OpenPorchSF_imputed:double,EnclosedPorch_imputed:double,3SsnPorch_imputed:double,ScreenPorch_imputed:double,PoolArea_imputed:double,MiscVal_imputed:double,MoSold_imputed:double,YrSold_imputed:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:161)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3943/0x0000000801b4dd48: (struct<MSSubClass:double,LotFrontage:double,LotArea:double,OverallQual:double,OverallCond:double,YearBuilt:double,YearRemodAdd:double,MasVnrArea:double,BsmtFinSF1:double,BsmtFinSF2:double,BsmtUnfSF:double,TotalBsmtSF:double,1stFlrSF:double,2ndFlrSF:double,LowQualFinSF:double,GrLivArea:double,BsmtFullBath:double,BsmtHalfBath:double,FullBath:double,HalfBath:double,BedroomAbvGr:double,KitchenAbvGr:double,TotRmsAbvGrd:double,Fireplaces:double,GarageYrBlt:double,GarageCars:double,GarageArea:double,WoodDeckSF:double,OpenPorchSF:double,EnclosedPorch:double,3SsnPorch:double,ScreenPorch:double,PoolArea:double,MiscVal:double,MoSold:double,YrSold:double,MSSubClass_imputed:double,LotArea_imputed:double,OverallQual_imputed:double,OverallCond_imputed:double,YearBuilt_imputed:double,YearRemodAdd_imputed:double,BsmtFinSF1_imputed:double,BsmtFinSF2_imputed:double,BsmtUnfSF_imputed:double,TotalBsmtSF_imputed:double,1stFlrSF_imputed:double,2ndFlrSF_imputed:double,LowQualFinSF_imputed:double,GrLivArea_imputed:double,BsmtFullBath_imputed:double,BsmtHalfBath_imputed:double,FullBath_imputed:double,HalfBath_imputed:double,BedroomAbvGr_imputed:double,KitchenAbvGr_imputed:double,TotRmsAbvGrd_imputed:double,Fireplaces_imputed:double,GarageCars_imputed:double,GarageArea_imputed:double,WoodDeckSF_imputed:double,OpenPorchSF_imputed:double,EnclosedPorch_imputed:double,3SsnPorch_imputed:double,ScreenPorch_imputed:double,PoolArea_imputed:double,MiscVal_imputed:double,MoSold_imputed:double,YrSold_imputed:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:161)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:875)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:875)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 26 more\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Feature Engineering and Transformation\n",
    "for col_name in [\"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\", \"GarageCars\", \"GarageArea\"]:\n",
    "    df_train_cleaned = df_train_cleaned.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
    "    df_test_cleaned = df_test_cleaned.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
    "\n",
    "categorical_cols = [col_name for col_name, dtype in df_train_cleaned.dtypes if dtype == \"string\"]\n",
    "numerical_cols = [col_name for col_name, dtype in df_train_cleaned.dtypes if dtype != \"string\" and col_name != \"Id\" and col_name != \"SalePrice\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"numerical_features\")\n",
    "df_train_assembled = assembler.transform(df_train_cleaned)\n",
    "df_test_assembled = assembler.transform(df_test_cleaned)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_numerical_features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(df_train_assembled)\n",
    "df_train_scaled = scaler_model.transform(df_train_assembled)\n",
    "df_test_scaled = scaler_model.transform(df_test_assembled)\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid='keep') for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=f\"{indexer.getOutputCol()}_encoded\") for indexer in indexers]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "df_train_encoded = pipeline.fit(df_train_scaled).transform(df_train_scaled)\n",
    "df_test_encoded = pipeline.fit(df_test_scaled).transform(df_test_scaled)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[f\"{col}_encoded\" for col in categorical_cols] + [\"scaled_numerical_features\"],\n",
    "                            outputCol=\"features\")\n",
    "df_train_final = assembler.transform(df_train_encoded)\n",
    "df_test_final = assembler.transform(df_test_encoded)\n",
    "\n",
    "df_train_final = df_train_final.select(\"Id\", \"features\", \"SalePrice\")\n",
    "df_test_final = df_test_final.select(\"Id\", \"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Model Training and Evaluation using Cross-Validation\n",
    "# Split data into training and validation sets\n",
    "train_data, validation_data = df_train_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize Linear Regression model\n",
    "lr = LinearRegression(featuresCol='features', labelCol='SalePrice', maxIter=100, regParam=0.1)\n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1, 0.5]).build()\n",
    "\n",
    "# Initialize CrossValidator\n",
    "evaluator = RegressionEvaluator(labelCol=\"SalePrice\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Train the model using CrossValidator\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "validation_predictions = cvModel.transform(validation_data)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "rmse = evaluator.evaluate(validation_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on validation data: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Model Prediction on Test Data and Save Results\n",
    "# Make predictions on the test set\n",
    "test_predictions = cvModel.transform(df_test_final)\n",
    "\n",
    "# Select the necessary columns for the final result\n",
    "final_result = test_predictions.select(\"Id\", \"prediction\").withColumnRenamed(\"prediction\", \"SalePrice\")\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "final_result.coalesce(1).write.csv(\"predictions.csv\", header=True, mode=\"overwrite\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
